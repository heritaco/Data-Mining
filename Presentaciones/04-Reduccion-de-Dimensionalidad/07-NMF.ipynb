{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "26087627",
      "metadata": {
        "id": "26087627"
      },
      "source": [
        "# **NONNEGATIVE MATRIX FACTORIZATION (NMF)**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54180431",
      "metadata": {
        "id": "54180431"
      },
      "source": [
        "## **INTRODUCTION**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16360890",
      "metadata": {
        "id": "16360890"
      },
      "source": [
        "In the **PCA** as well as the **SVD** analysis, the underlying constraints were the **orthogonality** of the involved **basis vectors**, in the PCA, and of the column vectors in the U and V matrices in the SVD.\n",
        "\n",
        "**PCA** is formulated as a task **minimizing** the mean square error subject to the orthogonality constraint of the basis vectors.\n",
        "\n",
        "Although, in some cases, the resulting expansion is useful, for some other cases such a constraint turns out to be very *weak* in representing the data.\n",
        "\n",
        "More recently, a new **matrix factorization** was\n",
        "suggested in [Paat (1991)](https://www.sciencedirect.com/science/article/abs/pii/S0021850205800898) and [Paat (1991)](https://onlinelibrary.wiley.com/doi/abs/10.1002/env.3170050203), which guarantees the **nonnegativity** of the elements of the resulting matrix factors. Such a constraint is enforced in certain applications since negative elements contradict physical reality. For example, in **image analysis** the intensity values of the pixels cannot be negative. Also, probability values cannot be negative."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26b70deb",
      "metadata": {
        "id": "26b70deb"
      },
      "source": [
        "The resulting factorization is known as **Nonnegative Matrix Factorization (NMF)** and it has been used successfully in a number of\n",
        "applications:\n",
        "- Document clustering\n",
        "- Molecular pattern discovery\n",
        "- Image analysis\n",
        "- Clustering\n",
        "- Music transcriptio\n",
        "- Music instrument classification\n",
        "- Face verification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "059f3937",
      "metadata": {
        "id": "059f3937"
      },
      "source": [
        "## **MATHEMATICAL DETAILS**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de8609b",
      "metadata": {
        "id": "0de8609b"
      },
      "source": [
        "Given a $l\\times n$ matrix $X$, the task of **NMF** consists of finding an **approximate factorization** of $X$, that is,\n",
        "\n",
        "$$X \\approx WH$$\n",
        "\n",
        "where $W$ and $H$ are $l\\times r$  and $r\\times n$ matrices, respectively, $r<\\min(n, l)$ and\n",
        "**all the matrix elements** are **nonnegative**, that is,\n",
        "\\begin{eqnarray}\n",
        "W(i,k) &\\geq& 0\\\\\n",
        "H(k,j) &\\geq& 0\n",
        "\\end{eqnarray}\n",
        "\n",
        "$i=1,2\\ldots, l$, $k=1,2,\\ldots,r$, $j=1,2,\\ldots,n$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d027a1d1",
      "metadata": {
        "id": "d027a1d1"
      },
      "source": [
        "Clearly, matrices $W$ and $H$ are of **rank at most $r$** and their product is a low rank, at most r, **approximation** of $X$.\n",
        "\n",
        "The significance of the above is that every **column vector** in $X$ is represented by the\n",
        "expansion:\n",
        "\n",
        "$$\\textbf{x}_i = \\sum_{k=1}^r H(k,i) \\textbf{w}_k, \\quad i=1,2,\\ldots,n$$\n",
        "\n",
        "where $\\textbf{w}_k$ are the column vectors fo $W$ and constitute the **basis** of the **expansion**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0da1fc7a",
      "metadata": {
        "id": "0da1fc7a"
      },
      "source": [
        "The number of vectors in the basis is less than the dimensionality\n",
        "of the vector itself. Hence, **NMF** can also be seen as a method for dimensionality\n",
        "**reduction**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff02a38",
      "metadata": {
        "id": "dff02a38"
      },
      "source": [
        "To get a good approximation of $X\\approx WH$ one can adopt different costs. The most\n",
        "conventional cost is the (squared) **Frobenius norm** of the **error matrix**. In such a setting, the **NMF task** is casted as follows:\n",
        "\n",
        "\\begin{eqnarray}\n",
        "\\min_{W, H} ||X-WH||_F^2 & \\equiv & \\sum_{i=1}^l \\sum_{j=1}^n \\left(X(i,j) - WH(i,j) \\right)^2\\\\\n",
        "\\text{s.t} & & W(i,k) \\geq 0 \\qquad \\forall i,k\\\\\n",
        " & & H(k,j) \\geq 0 \\qquad \\forall k,j\\\\\n",
        "\\end{eqnarray}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b8d4ca6",
      "metadata": {
        "id": "3b8d4ca6"
      },
      "source": [
        "where $WH(i, j)$ is the $(i, j)$ element of matrix $WH$. Minimization is performed\n",
        "with respect to $W$ and $H$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2302c2ba",
      "metadata": {
        "id": "2302c2ba"
      },
      "source": [
        "Another cost function has also been suggested, which is a close relative of the **Kullback–Leibler distance** and the objective function now becomes\n",
        "\n",
        "$$\\sum_{i=1}^l \\sum_{j=1}^n \\left(X(i,j) \\ln{\\frac{X(i,j)}{WH(i,j)}} - X(i,j)+WH(i,j) \\right)$$\n",
        "\n",
        "It is readily seen that if $X=WH$ the previous cost becomes zero.\n",
        "\n",
        "Note, however, that this cost  function is not well defined if either $X(i, j)$ or $WH(i, j)$ are zero."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e33807a",
      "metadata": {
        "id": "5e33807a"
      },
      "source": [
        "Once thhe problem has been formulated, the major issue rests on the solution of the **optimization task**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72df886d",
      "metadata": {
        "id": "72df886d"
      },
      "source": [
        "## **IMPLEMENTATION**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aaae89d",
      "metadata": {
        "id": "1aaae89d"
      },
      "source": [
        "The *scikit-learn** library offers the method\n",
        "\n",
        "> sklearn.decomposition.NMF()\n",
        "\n",
        "It use the following objective function\n",
        "\n",
        "$$L(W,H) = \\frac{1}{2}||X-WH||_{loss}^2 + \\alpha_W \\beta  n ||v(W)||_1 +\\frac{1}{2}\\alpha_W (1-\\beta)n||W||_F^2 + \\alpha_H \\beta  l ||v(H)||_1  +\\frac{1}{2}\\alpha_H (1-\\beta)l||H||_F^2$$\n",
        "\n",
        "where $||A||_F^2=\\sum_{i,j} A(i,j)^2$ (**Frobenius norm**) and $||v(A)||_1= \\sum_{i,j} |A(i,j)|$ (**elementwise $L_1$ norm**)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6006235f",
      "metadata": {
        "id": "6006235f"
      },
      "source": [
        "#### **Parameters**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02c5426a",
      "metadata": {
        "id": "02c5426a"
      },
      "source": [
        "> beta_loss\n",
        "\n",
        "It control the generic norm $||X-WH|||_{loss}$. The options are:\n",
        "* ‘frobenius’ (default)\n",
        "* ‘kullback-leibler’\n",
        "* itakura-saito’\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "791c1f30",
      "metadata": {
        "id": "791c1f30"
      },
      "source": [
        "> solver\n",
        "\n",
        "It decides the numerical solver to optimization task."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ece4574",
      "metadata": {
        "id": "6ece4574"
      },
      "source": [
        "> alpha_W\n",
        "\n",
        "> alpha_H\n",
        "\n",
        "They are the constants that multiplies the regularization terms. Set it to zero (default) to have no regularization on $W$ or $W$, respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8999b79",
      "metadata": {
        "id": "a8999b79"
      },
      "source": [
        "> l1_ratio\n",
        "\n",
        "In the function refered as $\\beta$\n",
        "* For $\\text{l1_ratio}=0$ the penalty is an elementwise $L_2$ penalty (**Frobenius Norm**).\n",
        "* For $\\text{l1_ratio} = 1$ it is an elementwise $L_1$ penalty.\n",
        "* For $0 < \\text{l1_ratio} < 1$, the penalty is a combination of $L_1$ and $L_2$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c2a09e",
      "metadata": {
        "id": "13c2a09e"
      },
      "source": [
        "## **APPENDICES**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61972421",
      "metadata": {
        "id": "61972421"
      },
      "source": [
        "----\n",
        "\n",
        "### **Frobenius norm**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4754dca8",
      "metadata": {
        "id": "4754dca8"
      },
      "source": [
        "\n",
        "\n",
        "The **Frobenius norm** is a special case of the $L_{p,q}$ norm of matrices. This norm can be defined in various ways:\n",
        "\n",
        "$$||A||_F = \\sqrt{\\sum_{i=1}^m\\sum_{j=1}^n |a_{ij}|^2} = \\sqrt{\\text{Tr}(A^TA)}$$\n",
        "\n",
        "where the **trace ($Tr$)** is the sum of diagonal entries of the matrix $A$.\n",
        "\n",
        "The **Frobenius norm** is an extension of the **Euclidean norm** to $K^{n\\times n}$ and comes from the **Frobenius inner product** on the space of all matrices, defined as\n",
        "$$<A,B>_F = \\text{Tr}(A^T B)$$\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f3882fd",
      "metadata": {
        "id": "0f3882fd"
      },
      "source": [
        "----\n",
        "\n",
        "### **Kullback-Leibler distance**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42d80708",
      "metadata": {
        "id": "42d80708"
      },
      "source": [
        "The **Kullback–Leibler (KL)** distance is a measure of the distance between two **probability density functions** $p(x)$ and $q(x)$ and is defined as\n",
        "$$L = -\\int p(x) \\ln{\\frac{q(x)}{p(x)}}dx$$\n",
        "\n",
        "Sometimes it is referred to as **cross** or **relative entropy** or **KL divergence**.\n",
        "\n",
        "The KL distance can be shown to be always **nonnegative** but it **is not a true distance measure**, from a mathematical point of view, since it **is not symmetric**.\n",
        "\n",
        "The **KL distance** is closely related to the **mutual information measure**, $I$ , between $l$ scalar random variables, $x_i$, $i=1, 2, \\ldots, l$.\n",
        "\n",
        "Indeed, let us compute the **KL distance** between the joint pdf $p(x)$ and the pdf resulting from the product of the corresponding marginal probability densities, that is,\n",
        "\n",
        "\\begin{eqnarray}\n",
        "L &=& -\\int p(x) \\ln{\\frac{\\Pi_{i=1}^l p_i(x_i)}{p(x)}}dx\\\\\n",
        "&=& -\\int p(x) \\left( \\sum_{i=1}^l \\ln{p_i}(x_i) - \\ln{p(x)}\\right)dx\\\\\n",
        "&=& \\int p(x) \\ln{p(x)}dx- \\sum_{i=1}^l \\int p(x) \\ln{p_i}(x_i) dx\\\\\n",
        "&=& -H(x) - \\sum_{i=1}^l \\int p(x) \\ln{p_i}(x_i) dx\n",
        "\\end{eqnarray}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4441d189",
      "metadata": {
        "id": "4441d189"
      },
      "source": [
        "Carrying out the integrations on the right-hand side it is straightforward to see the **KL distance** is equal to the **mutual information**, $I$ , defined as\n",
        "\n",
        "$$I(x_1,x_2,\\ldots,x_l)= -H(x) + \\sum_{i=1}^l H(x_i)$$\n",
        "\n",
        "where H(xi) is the **associated entropy** of $x_i$ , defined as\n",
        "\n",
        "$$H(x_i) = - \\int p_i(x_i) \\ln{p_i}(x_i) dx$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7bde389",
      "metadata": {
        "id": "a7bde389"
      },
      "source": [
        "\n",
        "----------"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77103076",
      "metadata": {
        "id": "77103076"
      },
      "source": [
        "## **REFERENCES**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a721201",
      "metadata": {
        "id": "5a721201"
      },
      "source": [
        "*  Theodoridis, S. \\& Koutroumbas, K. *Pattern Recognition*, 4th ed., Academic Press, 2009\n",
        "\n",
        "* Sra, \\& Dhillon (2006) *Nonnegative matrix approximation: Algorithms and applications.* Technical Report TR-06-27, University of Texas at Austin."
      ]
    }
  ],
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}